{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator\n",
    "Based on https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version 2.6.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import keras\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "print('tf version', tf.__version__)\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed precision (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "WARNING:tensorflow:From /home/benediktewallace/.local/lib/python3.6/site-packages/keras/mixed_precision/loss_scale.py:52: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n",
      "Loss scale: DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)\n",
      "keras backend float size before:  float32\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "\n",
      "Num GPUs Available:  2\n",
      "tf version  2.6.0\n",
      "keras version  2.6.0\n",
      "keras backend float size:  float16\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)\n",
    "\n",
    "loss_scale = policy.loss_scale\n",
    "print('Loss scale: %s' % loss_scale)\n",
    "\n",
    "\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "\n",
    "# set GPU config \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement=True\n",
    "config.allow_soft_placement=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction=0.8\n",
    "\n",
    "# set config for mixed precision\n",
    "# default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\n",
    "float_type = 'float16'\n",
    "print(\"keras backend float size before: \", K.floatx())#\n",
    "K.set_epsilon(1e-4) \n",
    "K.set_floatx(float_type)\n",
    "\n",
    "\n",
    "# set the new configs\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "G = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", G)\n",
    "print(\"tf version \", tf.__version__)\n",
    "print(\"keras version \", tf.keras.__version__)\n",
    "print(\"keras backend float size: \", K.floatx())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 300\n",
    "embed_dim = 32\n",
    "latent_dim = 256\n",
    "num_heads = 4\n",
    "\n",
    "SEQ_LEN = 300\n",
    "FEATS = 60\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Training sequences\n",
      "100 Validation sequences\n",
      "(100, 300, 60)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('data/x_train_small.npy')\n",
    "y_train = np.load('data/y_train_small.npy')\n",
    "x_val = np.load('data/x_val_small.npy')\n",
    "y_val = np.load('data/y_val_small.npy')\n",
    "x_test = np.load('data/x_test_small.npy')\n",
    "y_test = np.load('data/y_test_small.npy')\n",
    "\n",
    "\n",
    "\n",
    "#x_train = x_train[:8]\n",
    "#y_train = y_train[:8]\n",
    "#x_val = x_train\n",
    "#y_val = y_train\n",
    "\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_dataset(inp, targ):\n",
    "    return ({\"encoder_inputs\": inp, \"decoder_inputs\": targ,},targ)\n",
    "\n",
    "\n",
    "def make_dataset(x,y,bs):\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(bs)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(bs).cache()\n",
    "\n",
    "train_ds = make_dataset(x_train, y_train,BS)\n",
    "val_ds = make_dataset(x_val, y_val,BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (8, 300, 60)\n",
      "inputs[\"decoder_inputs\"].shape: (8, 300, 60)\n",
      "targets.shape: (8, 300, 60)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leasring rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\" linear warm up - linear decay \"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / (self.decay_epochs),\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        return self.calculate_lr(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        #\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim, input_length = SEQ_LEN)\n",
    "        \n",
    "        # token embedding of features will be merged so shape pf position embedding output dim must match \n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim*FEATS)\n",
    "        \n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        \n",
    "        # merging feature and embedding dimentions.\n",
    "        emb_shape = tf.shape(embedded_tokens) # (bs, seqlen, feats, embdim)\n",
    "        embedded_tokens = tf.reshape(embedded_tokens, [emb_shape[0], emb_shape[1],emb_shape[-2]*emb_shape[-1]])\n",
    "        #print(f\"shape of token emb reshaped: {embedded_tokens}\" ) # (bs, seqlen, feats*embdim)\n",
    "\n",
    "        length = tf.shape(embedded_tokens)[-2]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        \n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "\n",
    "        \n",
    "        ret = embedded_tokens + embedded_positions\n",
    "        return ret\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            # Ignoring this as all my sequences are of the same length\n",
    "            print(f\"encoder mask is not none: {mask}- but will be set to none\")\n",
    "            padding_mask = None\n",
    "            \n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        \n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            # Ignoring this as all my sequences are of the same length\n",
    "            print(f\"mask is not None: {mask}\\n but will be set to None\")\n",
    "            padding_mask = None\n",
    "        \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        \n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "        \n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        \n",
    "        \n",
    "        ret = self.layernorm_3(out_2 + proj_output)\n",
    "        \n",
    "        # dealing with merged feat+emb dim\n",
    "        ret_shape = tf.shape(ret)\n",
    "        ret = tf.reshape(ret, (ret_shape[0], ret_shape[1], 60, embed_dim))\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder mask is not none: Tensor(\"Placeholder_1:0\", shape=(None, 300, 60), dtype=bool)- but will be set to none\n",
      "mask is not None: Tensor(\"Placeholder_2:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n",
      "mask is not None: Tensor(\"model_1/positional_embedding_1/NotEqual:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(SEQ_LEN,FEATS,)\n",
    "                , dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQ_LEN, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim*FEATS, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(SEQ_LEN,FEATS,)\n",
    "                                 , dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim*FEATS,), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQ_LEN, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim*FEATS, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)     \n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize = 8\n",
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 300, 60)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding (Positiona (None, None, 1920)   585600      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 300, 60)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, None, 1920)   60000256    positional_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, None, 60, 300 119606956   decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 180,192,812\n",
      "Trainable params: 180,192,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\n",
      "  opt = tf.keras.mixed_precision.LossScaleOptimizer(opt)\n",
      "\n",
      "Epoch 1/30\n",
      "encoder mask is not none: Tensor(\"transformer/positional_embedding/NotEqual:0\", shape=(None, 300, 60), dtype=bool)- but will be set to none\n",
      "mask is not None: Tensor(\"transformer/model_1/positional_embedding_1/NotEqual:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n",
      "encoder mask is not none: Tensor(\"transformer/positional_embedding/NotEqual:0\", shape=(None, 300, 60), dtype=bool)- but will be set to none\n",
      "mask is not None: Tensor(\"transformer/model_1/positional_embedding_1/NotEqual:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n",
      "     13/Unknown - 4s 115ms/step - loss: 5.6095 - accuracy: 0.0147encoder mask is not none: Tensor(\"transformer/positional_embedding/NotEqual:0\", shape=(None, 300, 60), dtype=bool)- but will be set to none\n",
      "mask is not None: Tensor(\"transformer/model_1/positional_embedding_1/NotEqual:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n",
      "13/13 [==============================] - 5s 201ms/step - loss: 5.6095 - accuracy: 0.0147 - val_loss: 5.3442 - val_accuracy: 0.0512\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.34422, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 2/30\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 5.4142 - accuracy: 0.0250 - val_loss: 5.2891 - val_accuracy: 0.0459\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.34422 to 5.28906, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 3/30\n",
      "13/13 [==============================] - 2s 164ms/step - loss: 5.3227 - accuracy: 0.0365 - val_loss: 5.1570 - val_accuracy: 0.0671\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.28906 to 5.15703, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 4/30\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 5.1834 - accuracy: 0.0576 - val_loss: 5.0358 - val_accuracy: 0.0818\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.15703 to 5.03578, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 5/30\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 5.0184 - accuracy: 0.0828 - val_loss: 4.8255 - val_accuracy: 0.1376\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.03578 to 4.82547, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 6/30\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 4.8058 - accuracy: 0.1252 - val_loss: 4.6264 - val_accuracy: 0.2315\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.82547 to 4.62641, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 7/30\n",
      "13/13 [==============================] - 2s 166ms/step - loss: 4.5327 - accuracy: 0.1936 - val_loss: 4.2863 - val_accuracy: 0.3653\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.62641 to 4.28625, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 8/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 4.2052 - accuracy: 0.3016 - val_loss: 4.0603 - val_accuracy: 0.4366\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.28625 to 4.06031, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 9/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 3.9173 - accuracy: 0.4064 - val_loss: 3.6491 - val_accuracy: 0.6731\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.06031 to 3.64914, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 10/30\n",
      "13/13 [==============================] - 2s 166ms/step - loss: 3.9264 - accuracy: 0.3277 - val_loss: 3.7874 - val_accuracy: 0.4539\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.64914\n",
      "Epoch 11/30\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 3.9137 - accuracy: 0.2666 - val_loss: 3.5123 - val_accuracy: 0.6203\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.64914 to 3.51227, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 12/30\n",
      "13/13 [==============================] - 2s 166ms/step - loss: 3.6652 - accuracy: 0.3374 - val_loss: 3.0162 - val_accuracy: 0.7269\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.51227 to 3.01617, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 13/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 3.3741 - accuracy: 0.4099 - val_loss: 3.1350 - val_accuracy: 0.6873\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.01617\n",
      "Epoch 14/30\n",
      "13/13 [==============================] - 2s 166ms/step - loss: 3.2435 - accuracy: 0.4163 - val_loss: 2.7478 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.01617 to 2.74781, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 15/30\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 3.3376 - accuracy: 0.3187 - val_loss: 2.7753 - val_accuracy: 0.6728\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.74781\n",
      "Epoch 16/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 3.0399 - accuracy: 0.3950 - val_loss: 2.5648 - val_accuracy: 0.6882\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.74781 to 2.56477, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 17/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 2.7820 - accuracy: 0.4631 - val_loss: 2.0668 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.56477 to 2.06680, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 18/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 2.4816 - accuracy: 0.5495 - val_loss: 1.8461 - val_accuracy: 0.8394\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.06680 to 1.84613, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 19/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 2.1309 - accuracy: 0.6502 - val_loss: 1.6831 - val_accuracy: 0.8664\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.84613 to 1.68309, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 20/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 1.8841 - accuracy: 0.7071 - val_loss: 1.3802 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.68309 to 1.38023, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 21/30\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 1.6586 - accuracy: 0.7556 - val_loss: 1.0939 - val_accuracy: 0.9363\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.38023 to 1.09395, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 167ms/step - loss: 1.4086 - accuracy: 0.8156 - val_loss: 0.8100 - val_accuracy: 0.9549\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.09395 to 0.81000, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 23/30\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 1.2092 - accuracy: 0.8547 - val_loss: 0.6713 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.81000 to 0.67131, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 24/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 1.0088 - accuracy: 0.8946 - val_loss: 0.5700 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.67131 to 0.57002, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 25/30\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 0.8931 - accuracy: 0.9123 - val_loss: 0.4293 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.57002 to 0.42933, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 26/30\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 0.7470 - accuracy: 0.9357 - val_loss: 0.3380 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.42933 to 0.33800, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 27/30\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 0.6541 - accuracy: 0.9475 - val_loss: 0.2611 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.33800 to 0.26114, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 28/30\n",
      "13/13 [==============================] - 2s 169ms/step - loss: 0.5738 - accuracy: 0.9572 - val_loss: 0.2279 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.26114 to 0.22791, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 29/30\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 0.5221 - accuracy: 0.9633 - val_loss: 0.1784 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.22791 to 0.17840, saving model to models/checkpoint-transformer_translator.h5\n",
      "Epoch 30/30\n",
      "13/13 [==============================] - 2s 168ms/step - loss: 0.4608 - accuracy: 0.9686 - val_loss: 0.1458 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.17840 to 0.14581, saving model to models/checkpoint-transformer_translator.h5\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "print(f\"batchsize = {BS}\")\n",
    "\n",
    "learning_rate = CustomSchedule(\n",
    "    init_lr=0.0001,\n",
    "    lr_after_warmup=0.001,\n",
    "    final_lr=0.0001,\n",
    "    warmup_epochs=15,\n",
    "    decay_epochs=85,\n",
    "    steps_per_epoch= x_train.shape[0]/BS,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "checkpoint_filepath = 'models/checkpoint-transformer_translator.h5'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping, checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder mask is not none: Tensor(\"Placeholder_1:0\", shape=(None, 300, 60), dtype=bool)- but will be set to none\n",
      "mask is not None: Tensor(\"Placeholder_2:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n",
      "mask is not None: Tensor(\"model_3/positional_embedding_3/NotEqual:0\", shape=(None, 300, 60), dtype=bool)\n",
      " but will be set to None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reload saved model\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(SEQ_LEN,FEATS,)\n",
    "                                 , dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQ_LEN, vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "encoder_outputs = TransformerEncoder(embed_dim*FEATS, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(SEQ_LEN,FEATS,)\n",
    "                                 , dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim*FEATS,), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQ_LEN, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim*FEATS, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer_reload = keras.Model(\n",
    "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "transformer_reload.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300, 60)\n",
      "(1, 300, 60)\n",
      "encoder mask is not none: [[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]]- but will be set to none\n",
      "mask is not None: [[[ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  ...\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]\n",
      "  [ True  True  True ...  True  True  True]]]\n",
      " but will be set to None\n",
      "prediction shape: (1, 300, 60, 300)\n"
     ]
    }
   ],
   "source": [
    "inp = x_test[0:1,:]\n",
    "targ = y_test[0:1,:]\n",
    "print(inp.shape)\n",
    "print(targ.shape)\n",
    "\n",
    "\n",
    "pred = transformer_reload([inp, targ])\n",
    "print(f\"prediction shape: {pred.shape}\")\n",
    "predicted = np.argmax(pred, axis=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_ex_to_tsv(ex, fn):\n",
    "    num_frames = ex.shape[0]\n",
    "    \n",
    "    \n",
    "    freq = 30\n",
    "    marker_names = ['MARKER_NAMES','Head','neck','rsho','relb','rwri','rhan','lsho','lelb','lwri','lhan','back','root','rhip','lhip','rknee','lknee','rank','lank', 'rfoot', 'lfoot']\n",
    "    \n",
    "    with open('data/animations/'+fn+'.tsv', 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(['NO_OF_FRAMES', num_frames])\n",
    "        tsv_writer.writerow(['NO_OF_CAMERAS', 0])\n",
    "        tsv_writer.writerow(['NO_OF_MARKERS', 20])\n",
    "        tsv_writer.writerow(['FREQUENCY', freq])\n",
    "        tsv_writer.writerow(['NO_OF_ANALOG', 0])\n",
    "        tsv_writer.writerow(['ANALOG_FREQUENCY', 0])\n",
    "        tsv_writer.writerow(['DESCRIPTION--', ''])\n",
    "        tsv_writer.writerow(['TIME_STAMP--', ''])\n",
    "        tsv_writer.writerow(['DATA_INCLUDED', '3D'])\n",
    "        tsv_writer.writerow(marker_names)\n",
    "\n",
    "        \n",
    "        for frame in range(num_frames):\n",
    "            tsv_writer.writerow(ex[frame,:])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_ex_to_tsv(predicted[0], 'test-chollet3D-seqlen300step1-PRED')\n",
    "write_ex_to_tsv(inp[0], 'test-chollet3D-seqlen300step1-INPUT')\n",
    "write_ex_to_tsv(targ[0], 'test-chollet3D-seqlen300step1-TARGET')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
